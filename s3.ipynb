{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Yr7xR_VFKoi"
   },
   "source": [
    "# Домашка \n",
    "---------------------------------------\n",
    "\n",
    "tldr:\n",
    "    \n",
    "* Выбрать архитектуру из рассказанных NST, pix2pix, CycleGAN$^1$\n",
    "* Подберите к ней задачу, чтобы она вам нравилась\n",
    "* Подберите еще одну задачу, которая уже решена (если не NST)\n",
    "* Повторите решение, которое уже есть$^2$ (если не NST)\n",
    "* Решите свою задачу\n",
    "\n",
    "---------------------------------------\n",
    "1. Расположены в порядке возрастания сложности и крутизны\n",
    "2. Поверьте если вы сделаете этот пункт следующий будет в *разы* легче"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DiLO2hj1FKok"
   },
   "source": [
    "## Если вы выбрали Neural Style Transfer\n",
    "---------------------------------------\n",
    "Тут все довольно просто на первый и на второй взгляд. Поэтому недосотаточно просто написать свою функцию потерь и сдать ноутбук. Если вы хотите приличных баллов, то у вас есть две опции:\n",
    "\n",
    "\n",
    "1. Вы разделяете картинку на две части и переносите на них разные стили. <p><span style=\"color:red\">Нельзя просто взять и два раза применить обычную архитектуру сначала к одной чати картинки, а потом к другой.</span></p> От вас ожидается, что вы отдадите нейросети два картинки стиля и она внутри себя(скорее внутри лосс функции) разделит выходную картинку на две части и к одной части применит один стиль, а к другой - второй. \n",
    "\n",
    "2. Вы переносите *одновременно* два стиля на одну картинку контента.\n",
    "<p><span style=\"color:red\">Нельзя просто взять и два раза применить обычную архитектуру сначала с одним стилем, а потом с другим.</span></p>\n",
    "От вас ожидается, что вы модифицируете модель(скорее лосс модели) для того, чтобы два стиля учитывались с разными весами. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JP1Upi_TFKok"
   },
   "source": [
    "## Если вы выбрали pix2pix\n",
    "---------------------------------------------\n",
    "Здесь от вас ожидается, что вы реализуете свою архитектуру для pix2pix модели. Пожалуйста не копируйте код из открытых репозиториев. Этот факт очень легко обнаружить. Перед тем, как приступить проверьте, что обе задачи, которые вы выбрали влезают на вашу видеокарту или на карту Google Colab. Если они не влезают, но вам все равно очень хочется, то вы можете израсходовать все безплатные триалы облаков(Google, Amazon, .. etc) во вселенной. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNHLFscvFKol"
   },
   "source": [
    "## Если вы выбрали CycleGAN\n",
    "--------------------------------------------\n",
    "Здесь от вас ожидается, что вы реализуете свою архитектуру для CycleGAN модели. Пожалуйста не копируйте код из открытых репозиториев. Этот факт очень легко обнаружить. Перед тем, как приступить проверьте, что обе задачи, которые вы выбрали влезают на вашу видеокарту или на карту Google Colab. CycleGAN в этом смысле хуже, чем pix2pix, он ест больше памяти. Если они не влезают, но вам все равно очень хочется, то вы можете израсходовать все беcплатные триалы облаков(Google, Amazon, .. etc) во вселенной. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yRqgNI_FKom"
   },
   "source": [
    "## Remarks:\n",
    "-----------------------------------------\n",
    "\n",
    "* Это задание нужно для того, чтобы вы наступили на все грабли, что есть. Узнали об их существовании и научились обходить. Посмотрели на неработающие модели и поняли, что все тлен. Изгуглили весь интернет и в конце заставили это все работать. Поверьте, оно того стиот. Не откладывайте это задание на ночь перед сдачей, так как весь смысл \\*пуф\\* улетучится.\n",
    "\n",
    "* У вас два союзника в этой борьбе:\n",
    "    1. Оригинальная статья, те психи, что ее писала как то заставили свою модель работать. Их мысли, которыми они спроводили свое детище, позволят вам написать свой вариант алгоритма. \n",
    "    2. Гугл, он знает ответы на почти все ваши вопросы, но у него есть две ипостаси одна простая в обещении и вы все ее занаете(русскоязычная), а есть еще одна, которая кусается, но знает больше(англоязычная). Если не знаете языва - учите на ходу :)\n",
    "    \n",
    "* На самом деле у вас есть еще один союзник, это ментор проекта(или лектор или семинарист). Его ресурсом нужно пользоваться в ситуации, в которой вы не можете(занчит попытались и не вышло) найти ответов, используя Гугл и статью.\n",
    "\n",
    "* Сдавать это все нужно следующим образом. Код вы кидаете на github и отправляете ссылку туда, куда вам сказали(в телеграм, степик или еще куда-то)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_93ER8CSFKon"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR_B = Path('/home/muaddib/Documents/DL/HW19/facades/trainA')\n",
    "TEST_DIR_B = Path('/home/muaddib/Documents/DL/HW19/facades/testA')\n",
    "TRAIN_DIR_A = Path('/home/muaddib/Documents/DL/HW19/facades/trainB')\n",
    "TEST_DIR_A = Path('/home/muaddib/Documents/DL/HW19/facades/testB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from utils import PicDataset\n",
    "\n",
    "trainA = sorted(list(TRAIN_DIR_A.rglob('*.jpg')))\n",
    "trainB = sorted(list(TRAIN_DIR_B.rglob('*.jpg')))\n",
    "\n",
    "testA = sorted(list(TEST_DIR_A.rglob('*.jpg')))\n",
    "testB = sorted(list(TEST_DIR_B.rglob('*.jpg')))\n",
    "\n",
    "\n",
    "data = list(zip(trainA ,trainB ))\n",
    "\n",
    "test_files = [list(l) for l in list(zip(testA ,testB ))]\n",
    "\n",
    "train_files, val_files = train_test_split(data , test_size=0.25)\n",
    "\n",
    "print(len(train_files), len(val_files), len(test_files) )\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_dataset = PicDataset(train_files)\n",
    "val_dataset = PicDataset(val_files)\n",
    "test_dataset = PicDataset(test_files)\n",
    "\n",
    "#data_tr = DataLoader(train_dataset ,batch_size=batch_size, shuffle=True)\n",
    "#data_val = DataLoader(val_dataset ,batch_size=batch_size, shuffle=True)\n",
    "#data_ts = DataLoader(test_dataset ,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#print(len(data_tr), len(data_val), len(data_ts) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "for i in range(6):\n",
    "    X, Y = val_dataset[i] \n",
    "    plt.subplot(2, 6, i+1)\n",
    "    plt.axis(\"off\")    \n",
    "    X = X.numpy().transpose((1, 2, 0))\n",
    "    X = (X-X.min())/(X.max()-X.min()) \n",
    "    plt.imshow(X)\n",
    "\n",
    "    plt.subplot(2, 6, i+7)\n",
    "    plt.axis(\"off\")\n",
    "    Y = Y.numpy().transpose((1, 2, 0))\n",
    "    Y = (Y-Y.min())/(Y.max()-Y.min()) \n",
    "    plt.imshow(Y/(Y.max() - Y.min()) - Y.min())\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossGan(yPred, yReal, eps = 1e-7):\n",
    "    yReal = (yReal - yReal.min())/(yReal.max() - yReal.min())\n",
    "    yPred = (yPred - yPred.min())/(yPred.max() - yPred.min())\n",
    "    return  torch.mean(torch.log(eps+yReal)) + torch.mean(torch.log(eps + 1.0 - yPred))\n",
    "    #return torch.mean(torch.log(eps+torch.sigmoid(yReal))) + torch.mean(torch.log(eps + 1.0 - torch.sigmoid(yPred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import generatorNet\n",
    "from models import discriminatorNet\n",
    "\n",
    "gen = generatorNet().to(device)\n",
    "dsc = discriminatorNet().to(device)\n",
    "\n",
    "optimG = torch.optim.Adam(filter(lambda p: p.requires_grad, gen.parameters()),lr=0.0003)\n",
    "optimD = torch.optim.Adam(filter(lambda p: p.requires_grad, dsc.parameters()),lr=0.0003)\n",
    "\n",
    "#criterionGAN = nn.BCEWithLogitsLoss()\n",
    "criterionGAN=lossGan\n",
    "criterionL1 = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, data_tr, data_val, lambdaL1):\n",
    "    train_loader = DataLoader(data_tr ,batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(data_val ,batch_size=batch_size, shuffle=True)\n",
    "    historyLossD = []\n",
    "    historyLossG = []\n",
    "    \n",
    "    A_val, B_val = next(iter(val_loader))\n",
    "    \n",
    "    for epoch in range(epochs):  \n",
    "        avg_lossD = 0\n",
    "        avg_lossG = 0\n",
    "        gen.train()\n",
    "        for A, B in train_loader:\n",
    "            \n",
    "            A = A.to(device)\n",
    "            B = B.to(device)            \n",
    "            \n",
    "            optimD.zero_grad()\n",
    "            \n",
    "            generatedB = gen(A)            \n",
    "           \n",
    "            for param in dsc.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            generatedAB = torch.cat((A, generatedB), 1)\n",
    "            predGenerated = dsc(generatedAB.detach())\n",
    "            \n",
    "            lossDGenerated = criterionL1(predGenerated, torch.ones_like(predGenerated))\n",
    "            \n",
    "            realAB = torch.cat((A, B), 1)\n",
    "            predReal = dsc(realAB.detach())\n",
    "            \n",
    "            lossDReal = criterionL1(predReal, torch.zeros_like(predReal))\n",
    "            \n",
    "            lossD = (lossDGenerated + lossDReal) / 2\n",
    "            \n",
    "            lossD.backward(retain_graph=True)\n",
    "            \n",
    "            optimD.step()\n",
    "           \n",
    "            \n",
    "            optimG.zero_grad()\n",
    "            \n",
    "            for param in dsc.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            predGenerated = dsc(generatedAB.detach())\n",
    "            \n",
    "            realAB = torch.cat((A, B), 1)\n",
    "            \n",
    "            predReal = dsc(realAB.detach())\n",
    "            \n",
    "            lossG_GAN = criterionGAN(predGenerated, predReal)\n",
    "            \n",
    "            lossG_L1 = criterionL1(generatedB, B) * lambdaL1\n",
    "            \n",
    "            lossG = lossG_GAN + lossG_L1\n",
    "            \n",
    "            lossG.backward()\n",
    "            \n",
    "            optimG.step()\n",
    "\n",
    "            avg_lossD += float(lossD) / len(train_loader)\n",
    "            avg_lossG += float(lossG) / len(train_loader)\n",
    "            \n",
    "        print('lossD: %f' % avg_lossD,\";  \", 'lossG: %f' % avg_lossG )\n",
    "        historyLossD.append(avg_lossD)\n",
    "        historyLossG.append(avg_lossG)\n",
    "        gen.eval() \n",
    "        B_hat = gen(A_val.to(device)).detach().cpu()\n",
    "        B_val = (B_val-B_val.min())/(B_val.max()-B_val.min()) \n",
    "        B_hat = (B_hat-B_hat.min())/(B_hat.max()-B_hat.min())         \n",
    "        if epoch % 50 == 0:\n",
    "            plt.figure(figsize=(18, 6))\n",
    "            for k in range(6):\n",
    "                plt.subplot(2, 6, k+1)\n",
    "                plt.imshow(np.rollaxis(B_val[k].numpy(), 0, 3), cmap='gray')\n",
    "                plt.title('Real')\n",
    "                plt.axis('off')            \n",
    "                plt.subplot(2, 6, k+7)            \n",
    "                plt.imshow(np.rollaxis(B_hat[k].numpy(), 0, 3), cmap='gray')\n",
    "                plt.title('Output')\n",
    "                plt.axis('off')        \n",
    "            plt.show()    \n",
    "    return historyLossD, historyLossG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(501, train_dataset, val_dataset, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW_NST_pix2pix_CycleGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
